"""
LangChain Agent æœåŠ¡
- åŸºäº LangChain çš„å®Œæ•´ Agent å®ç°
- é›†æˆå‘é‡å­˜å‚¨æœåŠ¡è¿›è¡ŒçŸ¥è¯†æ£€ç´¢
- é›†æˆå­¦ä¹ æœåŠ¡è¿›è¡Œè‡ªä¸»å­¦ä¹ 
- æ”¯æŒå·¥å…·è°ƒç”¨å’Œå¤æ‚æ¨ç†
- æä¾›ä¸ç»Ÿä¸€ Agent æœåŠ¡ç›¸åŒçš„æ¥å£
"""

import asyncio
import json
from typing import Any, Dict, List, Optional

from loguru import logger

from ..config import config
from ..infra.vector import VectorService
from ..services.learning_service import LearningService


class LangChainAgentService:
    """åŸºäº LangChain çš„å®Œæ•´ Agent æœåŠ¡"""

    def __init__(
        self,
        database_service=None,
        mcp_service=None,
        vector_store_path: str = "data/vector_store",
    ):
        self.database_service = database_service
        self.mcp_service = mcp_service

        # æ ¸å¿ƒæœåŠ¡ç»„ä»¶
        self.vector_service = VectorService(database_service, vector_store_path)
        self.learning_service = None  # å°†åœ¨åˆå§‹åŒ–æ—¶åˆ›å»º

        # LangChain ç»„ä»¶
        self._llm = None
        self._agent = None
        self._tools = []
        self._retriever_tool = None

        # çŠ¶æ€
        self._initialized = False
        self._current_conversation_id = None

    async def initialize(self) -> bool:
        """åˆå§‹åŒ– LangChain Agent æœåŠ¡"""
        try:
            # åˆå§‹åŒ–å‘é‡æœåŠ¡
            logger.info("æ­£åœ¨åˆå§‹åŒ–å‘é‡æœåŠ¡...")

            # ç¡®å®šåµŒå…¥æ¨¡å‹ç±»å‹
            embedding_model_type = "tfidf"  # é»˜è®¤å€¼
            if hasattr(config, "agent") and hasattr(config.agent, "embedding_model"):
                embedding_model_type = config.agent.embedding_model
            elif hasattr(config, "langchain") and hasattr(
                config.langchain, "embedding_model_name"
            ):
                embedding_model_type = config.langchain.embedding_model_name

            # ç¡®å®šå‘é‡å­˜å‚¨åç«¯
            vector_store_backend = "memory"  # é»˜è®¤å€¼
            if hasattr(config, "agent") and hasattr(config.agent, "vector_store"):
                vector_store_backend = config.agent.vector_store
            elif hasattr(config, "langchain") and hasattr(
                config.langchain, "vector_store"
            ):
                vector_store_backend = config.langchain.vector_store

            vector_success = await self.vector_service.initialize(
                embedding_model_type=embedding_model_type,
                vector_store_backend=vector_store_backend,
            )

            if not vector_success:
                logger.error("å‘é‡æœåŠ¡åˆå§‹åŒ–å¤±è´¥")
                return False

            # åˆå§‹åŒ– LLM
            logger.info("æ­£åœ¨åˆå§‹åŒ– LangChain LLM...")
            from langchain_openai import ChatOpenAI

            self._llm = ChatOpenAI(
                model=config.openai.model,
                temperature=config.openai.temperature,
                openai_api_key=config.openai.api_key,
                base_url=config.openai.base_url,
                timeout=config.openai.timeout,
            )

            # åˆå§‹åŒ–å­¦ä¹ æœåŠ¡
            self.learning_service = LearningService(
                self.database_service,
                self.vector_service,
                self._create_llm_adapter(),  # åˆ›å»º LLM é€‚é…å™¨
            )

            # åˆå§‹åŒ–å·¥å…·
            await self._initialize_tools()

            # æ„å»º Agent
            await self._build_agent()

            self._initialized = True
            logger.info("LangChain Agent æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"LangChain Agent æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def _create_llm_adapter(self):
        """åˆ›å»º LLM é€‚é…å™¨ï¼Œç”¨äºå­¦ä¹ æœåŠ¡"""

        class LLMAdapter:
            def __init__(self, langchain_llm):
                self.langchain_llm = langchain_llm

            async def chat_completion(self, messages, **kwargs):
                """é€‚é…å­¦ä¹ æœåŠ¡çš„ chat_completion æ¥å£"""
                try:
                    # å°†æ¶ˆæ¯è½¬æ¢ä¸º LangChain æ ¼å¼
                    from langchain_core.messages import HumanMessage, SystemMessage

                    langchain_messages = []
                    for msg in messages:
                        if msg["role"] == "system":
                            langchain_messages.append(
                                SystemMessage(content=msg["content"])
                            )
                        elif msg["role"] == "user":
                            langchain_messages.append(
                                HumanMessage(content=msg["content"])
                            )

                    # è°ƒç”¨ LangChain LLM
                    response = await self.langchain_llm.ainvoke(langchain_messages)
                    return response.content

                except Exception as e:
                    logger.error(f"LLM é€‚é…å™¨è°ƒç”¨å¤±è´¥: {e}")
                    return None

        return LLMAdapter(self._llm)

    async def _initialize_tools(self):
        """åˆå§‹åŒ– Agent å·¥å…·"""
        try:
            from langchain_core.tools import Tool

            # åˆ›å»ºçŸ¥è¯†æ£€ç´¢å·¥å…·
            self._retriever_tool = Tool(
                name="knowledge_search",
                description="æœç´¢ç”¨æˆ·ç›¸å…³çš„ä¸ªäººä¿¡æ¯ã€å†å²å¯¹è¯è®°å½•ã€ç”Ÿæ—¥ã€å–œå¥½ç­‰ç§äººæ•°æ®ã€‚å½“ç”¨æˆ·è¯¢é—®å…³äºè‡ªå·±çš„ä¿¡æ¯ã€ç”Ÿæ—¥ã€ä¸ªäººè®¾ç½®æˆ–å†å²è®°å½•æ—¶ï¼Œå¿…é¡»ä½¿ç”¨æ­¤å·¥å…·ã€‚è¾“å…¥åº”è¯¥æ˜¯æœç´¢å…³é”®è¯ã€‚",
                func=self._search_knowledge_sync,
            )

            self._tools = [self._retriever_tool]

            # æ·»åŠ  MCP å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.mcp_service:
                mcp_tools = await self._create_mcp_tools()
                self._tools.extend(mcp_tools)

            logger.info(f"åˆå§‹åŒ–äº† {len(self._tools)} ä¸ªå·¥å…·")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å·¥å…·å¤±è´¥: {e}")

    async def _create_mcp_tools(self) -> List:
        """åˆ›å»º MCP å·¥å…·"""
        try:
            from langchain_core.tools import Tool

            tools = []
            available_tools = self.mcp_service.get_available_tools()

            if not available_tools:
                return tools

            for tool_name, tool_info in available_tools.items():
                # åˆ›å»º LangChain å·¥å…·åŒ…è£…å™¨
                def make_mcp_tool(name: str):
                    def mcp_tool_func(query: str) -> str:
                        try:
                            # å°è¯•è§£æå‚æ•°
                            try:
                                args = json.loads(query)
                            except json.JSONDecodeError:
                                args = {"query": query}

                            # åœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥ MCP è°ƒç”¨
                            import asyncio

                            try:
                                loop = asyncio.get_running_loop()
                                # å¦‚æœåœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œä½¿ç”¨ run_coroutine_threadsafe
                                future = asyncio.run_coroutine_threadsafe(
                                    self.mcp_service.call_tool(name, args), loop
                                )
                                result = future.result(timeout=30)
                            except RuntimeError:
                                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œç›´æ¥è¿è¡Œ
                                result = asyncio.run(
                                    self.mcp_service.call_tool(name, args)
                                )

                            return result or f"å·¥å…· {name} æ‰§è¡Œå®Œæˆ"
                        except Exception as e:
                            return f"å·¥å…· {name} æ‰§è¡Œå¤±è´¥: {str(e)}"

                    return mcp_tool_func

                tool = Tool(
                    name=tool_name,
                    description=tool_info.get("description", f"MCPå·¥å…·: {tool_name}"),
                    func=make_mcp_tool(tool_name),
                )
                tools.append(tool)

            logger.info(f"åˆ›å»ºäº† {len(tools)} ä¸ª MCP å·¥å…·")
            return tools

        except Exception as e:
            logger.error(f"åˆ›å»º MCP å·¥å…·å¤±è´¥: {e}")
            return []

    def _search_knowledge_sync(self, query: str) -> str:
        """åŒæ­¥çŸ¥è¯†æœç´¢ï¼ˆä¾› LangChain å·¥å…·ä½¿ç”¨ï¼‰"""
        try:
            import asyncio
            import concurrent.futures

            def run_async_in_thread():
                """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°"""
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        self._search_knowledge_async(query)
                    )
                finally:
                    new_loop.close()

            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_async_in_thread)
                return future.result(timeout=30)

        except Exception as e:
            logger.error(f"åŒæ­¥çŸ¥è¯†æœç´¢å¤±è´¥: {e}")
            return f"çŸ¥è¯†æœç´¢å¤±è´¥: {str(e)}"

    async def _search_knowledge_async(self, query: str) -> str:
        """å¼‚æ­¥çŸ¥è¯†æœç´¢"""
        try:
            results = await self.vector_service.search_knowledge(query, top_k=5)

            if not results:
                return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çŸ¥è¯†ã€‚"

            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            formatted_results = []
            for i, result in enumerate(results[:3], 1):  # æœ€å¤šè¿”å›3ä¸ªç»“æœ
                title = result.get("title", "æ— æ ‡é¢˜")
                content = result.get("content", result.get("summary", ""))
                score = result.get("similarity_score", 0)

                formatted_results.append(
                    f"{i}. {title}\nå†…å®¹: {content[:200]}...\nç›¸ä¼¼åº¦: {score:.2f}"
                )

            return "\n\n".join(formatted_results)

        except Exception as e:
            logger.error(f"å¼‚æ­¥çŸ¥è¯†æœç´¢å¤±è´¥: {e}")
            return f"çŸ¥è¯†æœç´¢å¤±è´¥: {str(e)}"

    async def _build_agent(self):
        """æ„å»º LangChain Agent"""
        try:
            from langchain.agents import AgentExecutor, create_openai_tools_agent
            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

            # åˆ›å»º Agent æç¤ºæ¨¡æ¿
            prompt = ChatPromptTemplate.from_messages(
                [
                    (
                        "system",
                        """ä½ æ˜¯å¤©ç«¥çˆ±ä¸½ä¸ï¼Œä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå…·æœ‰æœç´¢ç”¨æˆ·ä¸ªäººä¿¡æ¯çš„èƒ½åŠ›ã€‚

ğŸ” CRITICAL: å·¥å…·ä½¿ç”¨è§„åˆ™
1. å½“ç”¨æˆ·è¯¢é—®å…³äºè‡ªå·±çš„ä»»ä½•ä¿¡æ¯æ—¶ï¼Œå¿…é¡»é¦–å…ˆä½¿ç”¨ knowledge_search å·¥å…·æœç´¢
2. åŒ…æ‹¬ä½†ä¸é™äºï¼šç”Ÿæ—¥ã€ä¸ªäººå–œå¥½ã€å†å²å¯¹è¯ã€ä¸ªäººè®¾ç½®ã€ä¹‹å‰æåˆ°çš„ä¿¡æ¯
3. æœç´¢å…³é”®è¯ï¼šä»ç”¨æˆ·é—®é¢˜ä¸­æå–å…³é”®è¯ï¼Œå¦‚"ç”Ÿæ—¥"ã€"å–œå¥½"ã€"ä¿¡æ¯"ç­‰
4. åŸºäºæœç´¢ç»“æœå›ç­”ï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™è¯´æ˜æ²¡æœ‰ç›¸å…³è®°å½•

âš¡ è§¦å‘æœç´¢çš„é—®é¢˜ç±»å‹ï¼š
- "æˆ‘çš„ç”Ÿæ—¥æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ" â†’ æœç´¢"ç”Ÿæ—¥"
- "ä½ çŸ¥é“æˆ‘çš„ä¿¡æ¯å—ï¼Ÿ" â†’ æœç´¢"ç”¨æˆ·ä¿¡æ¯"  
- "æˆ‘ä¹‹å‰è¯´è¿‡ä»€ä¹ˆï¼Ÿ" â†’ æœç´¢"å†å²å¯¹è¯"
- "æˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ" â†’ æœç´¢"å–œå¥½"

è®°ä½ï¼šä½ æ˜¯å¤©ç«¥çˆ±ä¸½ä¸ï¼Œæ´»æ³¼å¯çˆ±ï¼Œç”¨"é‚¦é‚¦å¡é‚¦"ç­‰å£å¤´ç¦…ã€‚""",
                    ),
                    ("user", "{input}"),
                    MessagesPlaceholder(variable_name="agent_scratchpad"),
                ]
            )

            # åˆ›å»º Agent
            agent = create_openai_tools_agent(self._llm, self._tools, prompt)

            # åˆ›å»º Agent æ‰§è¡Œå™¨
            self._agent = AgentExecutor(
                agent=agent,
                tools=self._tools,
                verbose=True,
                max_iterations=5,
                max_execution_time=60,
                handle_parsing_errors=True,
            )

            logger.info("LangChain Agent æ„å»ºå®Œæˆ")

        except Exception as e:
            logger.error(f"æ„å»º Agent å¤±è´¥: {e}")
            # åˆ›å»ºç®€åŒ–çš„å›é€€æ–¹æ¡ˆ
            self._agent = None

    def set_conversation_id(self, conversation_id: int):
        """è®¾ç½®å½“å‰å¯¹è¯IDï¼Œç”¨äºå·¥å…·è°ƒç”¨è®°å½•"""
        self._current_conversation_id = conversation_id

    async def chat_completion(
        self, messages: List[Dict[str, str]], tools: List[Dict[str, Any]] = None
    ) -> Optional[str]:
        """èŠå¤©å®Œæˆæ¥å£ï¼ˆä¸ç»Ÿä¸€ Agent æœåŠ¡å…¼å®¹ï¼‰"""
        try:
            if not self._initialized:
                logger.warning("LangChain Agent æœåŠ¡æœªåˆå§‹åŒ–")
                return None

            # æå–ç³»ç»Ÿæ¶ˆæ¯ï¼ˆåŒ…å«è§’è‰²ä¿¡æ¯ï¼‰å’Œç”¨æˆ·æ¶ˆæ¯
            system_message = ""
            user_message = ""
            conversation_history = []

            for msg in messages:
                if msg["role"] == "system":
                    system_message = msg["content"]
                elif msg["role"] == "user":
                    user_message = msg["content"]
                elif msg["role"] == "assistant":
                    # ä¿å­˜å¯¹è¯å†å²ç”¨äºä¸Šä¸‹æ–‡
                    conversation_history.append(msg)

            if not user_message:
                return "æŠ±æ­‰ï¼Œæ— æ³•ä»æ¶ˆæ¯ä¸­æå–ç”¨æˆ·é—®é¢˜ã€‚"

            # å¦‚æœæœ‰ Agentï¼Œä½¿ç”¨ Agent å¤„ç†
            if self._agent:
                try:
                    # æ„å»ºåŒ…å«è§’è‰²ä¿¡æ¯çš„è¾“å…¥
                    agent_input = {"input": user_message}

                    # å¦‚æœæœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼ˆè§’è‰²ä¿¡æ¯ï¼‰ï¼Œæ·»åŠ åˆ°èŠå¤©å†å²ä¸­
                    if system_message:
                        # å°†è§’è‰²ä¿¡æ¯ä½œä¸ºé¢å¤–ä¸Šä¸‹æ–‡ä¼ é€’ç»™ Agent
                        enhanced_input = (
                            f"è§’è‰²è®¾å®š: {system_message}\n\nç”¨æˆ·é—®é¢˜: {user_message}"
                        )
                        agent_input = {"input": enhanced_input}

                    response = await self._agent.ainvoke(agent_input)
                    return response.get("output", "æŠ±æ­‰ï¼ŒAgent æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›ç­”ã€‚")
                except Exception as e:
                    logger.warning(f"Agent å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼: {e}")

            # ç®€åŒ–æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ LLM + çŸ¥è¯†æ£€ç´¢
            return await self._simple_rag_response(
                user_message, messages, system_message
            )

        except Exception as e:
            logger.error(f"LangChain Agent èŠå¤©å®Œæˆå¤±è´¥: {e}")
            return None

    async def _simple_rag_response(
        self,
        user_message: str,
        messages: List[Dict[str, str]],
        system_message: str = "",
    ) -> str:
        """ç®€åŒ–çš„ RAG å“åº”æ¨¡å¼"""
        try:
            # æœç´¢ç›¸å…³çŸ¥è¯†
            knowledge_results = await self.vector_service.search_knowledge(
                user_message, top_k=3
            )

            # æ„å»ºä¸Šä¸‹æ–‡
            context = ""
            if knowledge_results:
                context = "\nç›¸å…³çŸ¥è¯†:\n"
                for i, result in enumerate(knowledge_results, 1):
                    title = result.get("title", "æ— æ ‡é¢˜")
                    content = result.get("content", result.get("summary", ""))
                    context += f"{i}. {title}: {content[:200]}...\n"

            # æ„å»ºå¢å¼ºçš„æ¶ˆæ¯
            enhanced_messages = []

            # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ï¼ˆè§’è‰²ä¿¡æ¯ï¼‰
            if system_message:
                enhanced_messages.append({"role": "system", "content": system_message})

            # æ·»åŠ å¯¹è¯å†å²ï¼ˆé™¤äº†æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼‰
            for msg in messages[:-1]:
                if msg["role"] != "system":  # é¿å…é‡å¤æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
                    enhanced_messages.append(msg)

            # å¢å¼ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            enhanced_user_message = user_message
            if context:
                enhanced_user_message = f"{user_message}\n\n{context}"

            enhanced_messages.append({"role": "user", "content": enhanced_user_message})

            # è°ƒç”¨ LLM
            from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

            langchain_messages = []
            for msg in enhanced_messages:
                if msg["role"] == "system":
                    langchain_messages.append(SystemMessage(content=msg["content"]))
                elif msg["role"] == "user":
                    langchain_messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    langchain_messages.append(AIMessage(content=msg["content"]))

            response = await self._llm.ainvoke(langchain_messages)
            return response.content

        except Exception as e:
            logger.error(f"ç®€åŒ– RAG å“åº”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜ã€‚"

    async def analyze_image(
        self, image_data: bytes, prompt: str = "è¯·æè¿°è¿™å¼ å›¾ç‰‡"
    ) -> Optional[str]:
        """å›¾ç‰‡åˆ†æåŠŸèƒ½"""
        try:
            # ä½¿ç”¨ OpenAI çš„è§†è§‰æ¨¡å‹
            import base64

            from openai import AsyncOpenAI

            client = AsyncOpenAI(
                api_key=config.openai.api_key, base_url=config.openai.base_url
            )

            # ç¼–ç å›¾ç‰‡
            image_base64 = base64.b64encode(image_data).decode("utf-8")

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            },
                        },
                    ],
                }
            ]

            # è°ƒç”¨ OpenAI API
            response = await client.chat.completions.create(
                model="gpt-4-vision-preview",
                messages=messages,
                max_tokens=config.openai.max_tokens,
            )

            if response.choices and response.choices[0].message:
                return response.choices[0].message.content

            return None

        except Exception as e:
            logger.error(f"å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
            return None

    async def learn_from_conversation(
        self,
        user_message: str,
        bot_response: str,
        conversation_id: int,
        user_id: int,
        conversation_context: List[Dict[str, str]] = None,
    ) -> int:
        """ä»å¯¹è¯ä¸­å­¦ä¹ ï¼ˆä½¿ç”¨å­¦ä¹ æœåŠ¡ï¼‰"""
        try:
            if not self.learning_service:
                logger.warning("å­¦ä¹ æœåŠ¡æœªåˆå§‹åŒ–")
                return 0

            return await self.learning_service.learn_from_conversation(
                user_message,
                bot_response,
                conversation_id,
                user_id,
                conversation_context,
            )

        except Exception as e:
            logger.error(f"ä»å¯¹è¯ä¸­å­¦ä¹ å¤±è´¥: {e}")
            return 0

    async def get_learning_statistics(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not self.learning_service:
                return {}

            return await self.learning_service.get_learning_statistics()

        except Exception as e:
            logger.error(f"è·å–å­¦ä¹ ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    async def optimize_knowledge_base(self) -> Dict[str, int]:
        """ä¼˜åŒ–çŸ¥è¯†åº“"""
        try:
            if not self.learning_service:
                return {}

            return await self.learning_service.optimize_knowledge_base()

        except Exception as e:
            logger.error(f"ä¼˜åŒ–çŸ¥è¯†åº“å¤±è´¥: {e}")
            return {}

    async def learn_from_conversation(
        self,
        user_message: str,
        bot_response: str,
        conversation_id: int = None,
        user_id: int = None,
    ):
        """ä»å¯¹è¯ä¸­å­¦ä¹ ï¼ˆä¸ç»Ÿä¸€ Agent æœåŠ¡å…¼å®¹ï¼‰"""
        try:
            if self.learning_service:
                await self.learning_service.learn_from_conversation(
                    user_message, bot_response, conversation_id, user_id
                )
                logger.info("LangChain Agent å®Œæˆå¯¹è¯å­¦ä¹ ")
            else:
                logger.debug("å­¦ä¹ æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å­¦ä¹ ")
        except Exception as e:
            logger.error(f"LangChain Agent å­¦ä¹ å¤±è´¥: {e}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.vector_service:
                self.vector_service.cleanup()

            if self.learning_service:
                self.learning_service.cleanup()

            if self._llm:
                del self._llm
                self._llm = None

            if self._agent:
                del self._agent
                self._agent = None

            self._tools.clear()

            logger.debug("LangChain Agent æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.debug(f"LangChain Agent æœåŠ¡èµ„æºæ¸…ç†å¤±è´¥: {e}")

    def __del__(self):
        """ææ„å‡½æ•°"""
        try:
            self.cleanup()
        except Exception:
            pass
